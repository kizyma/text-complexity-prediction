# Text Complexity Prediction POC
This Jupyter Notebook demonstrates how to train a text complexity prediction model using the Hugging Face Transformers library. The notebook includes steps for data preparation, model training, evaluation, and inference.

# Usage
The project is structured around the notebook "Projector Kaggle Competition" that guide you through the entire process of text complexity prediction:

1. Data Preparation: Load and preprocess the text complexity dataset.

2. Model Training: Fine-tune a pre-trained BERT model using the training dataset.

3. Evaluation: Evaluate the trained model's performance using the validation dataset.

4. Inference: Use the trained model to make predictions on new text excerpts.

# Additional Information
1. The notebook uses the Hugging Face Transformers library, which simplifies working with pre-trained language models.
2. Ensure you have the required Python packages installed by using the provided requirements.txt file.

# How to use trained model
1. Unzip model from Split archive in model folder
2. Load model from model folder, (model/pytorch_model.bin)
2. TBD